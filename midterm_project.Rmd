---
output:
  html_document: default
  pdf_document: default
  word_document: default
---
### Midterm Project: NYC and Surrounding Area Income Analysis
GROUP 7
Name: Tong Zhang
      Kunyu Zhang
      Li Zhu
Time: April 5th, 2018

#### Loading NYC Census data
```{r}
nyc_census <- read.csv("nyc_census.csv")
print('dimension of nyc:')
dim(nyc_census)
nyc <- nyc_census[complete.cases(nyc_census), ]
dim(nyc)
```
#### Cleaning/Manipulating data
- replace county names by numbers 1-5
```{r}
print("County: ")
unique(nyc$County)
nyc$County = as.numeric(factor(nyc$County), levels = c("Bronx", "Kings", "New York","Queens", "Richmond"))
nyc$Borough = as.numeric(factor(nyc$Borough), levels = c("Bronx", "Kings", "New York", "Queens", "Richmond"))
```
- Remove variables that are insignificant
```{r}
#########################delete variable###############################
d = c('CensusTract','Borough', 'Men', 'Women','White', 'TotalPop','Native', 'PrivateWork','Asian','ChildPoverty','Transit','FamilyWork', 'Employed','IncomePerCapErr','IncomeErr')
nyc = nyc[ , !(names(nyc) %in% d)]
dim(nyc)
```

#### Fitting simple linear regression model
1. Income linear regression model
```{r}
# linear regression
fit_med = lm(nyc$Income~ .-nyc$Income, data = nyc)
summary(fit_med)
par(mfrow=c(2,2))
plot(fit_med)
```
2. Income per capita linear regression model
```{r}
income_per_cap = nyc$IncomePerCap
fit_cap = lm(nyc$IncomePerCap ~ .-nyc$IncomePerCap, data = nyc)
summary(fit_cap)
par(mfrow=c(2,2))
plot(fit_cap)
```

#### Predicting Income Models

##### 2.Ridge regression using lambda regularized terms
```{r}
# ridge for income
library(glmnet)
grid=10^seq(10,-2,length=100)    #Grid of lambda values
x=model.matrix(nyc$Income ~ .,nyc)[,-1]
y=nyc$Income
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)   #Run ridge regression for this grid
dim(coef(ridge.mod))
#Split into training set and test set to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

#Now use CV to find best lambda
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0,nfolds=10)
plot(cv.out)
bestlam=cv.out$lambda.min   #Lambda with minimum MSE
print("Best lambda")
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
print("MSE:")
mean((ridge.pred-y.test)^2)  #Test MSE associated with smallest lambda
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:19,]  #Now get ridge coefficients for model with best lambda
```

##### 2.Lasso regression using lambda regularized terms
```{r}
# lasso for income
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
set.seed(1)
# use cross validataion
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)

# get best lambda
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
print("Test MSE")
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)

# get coeffcients
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:19,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

##### 3.Stepwise Forward Model 
- 19 initial predictors
- optimal to 11 predictors
```{r}
# stepwise for Income 
library(leaps)
dataset = data.frame(nyc)
print('dimision of the dataset')
dim(dataset)
dataset = dataset[ ,!(names(dataset) %in% "IncomePerCap")]
dim(dataset)

# Determine the best subsets of predictors using all data
b = regsubsets(Income~.,data = dataset, nvmax=19)
summary(b)
lm.best = summary(b)
which.min(lm.best$cp)
################################################################################
set.seed(1)
dataset = na.omit(dataset)
#create train and test sets
train=sample(c(TRUE,FALSE),nrow(nyc),rep=TRUE)
test=(!train)
#dataset
#Find best model by finding the error on the number of variables
regfit.fwd=regsubsets(Income~., data=dataset, nvmax=20,method='forward')
regfit.fwd.train=regsubsets(Income~., data=dataset[train,], nvmax = 20, method='forward')
test.mat=model.matrix(Income~.,data=dataset[test,])


val.errors=rep(NA,19)
for (i in 1:19){
  coef=coef(regfit.fwd.train,id=i)
  pred=test.mat[,names(coef)]%*%coef
  val.errors[i]=mean((dataset$Income[test]-pred)^2)
}
which.min(val.errors)
print('test errors' )
min(val.errors)
#best 7 predictors with the training dataset
coef(regfit.fwd.train,11)
#################################################################
# define predict function
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coef=coef(object,id=id)
  xvars=names(coef)
  mat[,xvars]%*%coef
}

#Cross Validate for best number of predictors in the full dataset
set.seed(1)
k=5
folds=sample(1:k,nrow(dataset),replace=TRUE)
cv.errors2=matrix(NA,k,19,dimnames=list(NULL,paste(1:19)))
for (j in 1:k){
  fwd.fit=regsubsets(Income~.,data=dataset[folds!=j,],nvmax=19)
  for (i in 1:19){
    pred=predict(fwd.fit,dataset[folds==j,],id=i)
    cv.errors2[j,i]=mean((dataset$Income[folds==j]-pred)^2)
  }
}
mean.cv.errors2=apply(cv.errors2,2,mean)
# how many features are most important
which.min(mean.cv.errors2)
print(min(mean.cv.errors2))
#Cross validated reults give us 10 best features.
coef(fwd.fit,11)
```

#### Predicting Income per capita Model 

##### 1.ridge regression using the best lambda term
```{r}
# ridge for income per capita
library(glmnet)
grid=10^seq(10,-2,length=100)    #Grid of lambda values
x=model.matrix(nyc$IncomePerCap ~ .,nyc)[,-1]
y=nyc$IncomePerCap
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)   #Run ridge regression for this grid
dim(coef(ridge.mod))
#Split into training set and test set to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

#Now use CV to find best lambda
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0,nfolds=10)
plot(cv.out)
bestlam=cv.out$lambda.min   #Lambda with minimum MSE
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
print("MSE:")
mean((ridge.pred-y.test)^2)  #Test MSE associated with smallest lambda
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:19,]  #Now get ridge coefficients for model with best lambda
```

##### 2.ridge regression using the best lambda term
```{r}
# lasso for income per capita
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
set.seed(1)
# use cross validataion
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)

# get best lambda
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
print("MSE:")
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)

# get coeffcients
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:19,]
lasso.coef
lasso.coef[lasso.coef!=0]
```


##### 3.Income per capita with stepwise
- 19 initial predictors
- optimal to 15 predictors
```{r}
# stepwise income per capita
library(leaps)
dataset2 = data.frame(nyc)
print('dimision of the dataset')
dim(dataset2)
dataset2 = dataset2[ ,!(names(dataset2) %in% "Income")]
dim(dataset2)

b = regsubsets(IncomePerCap~.,data = dataset2, nvmax=19)
summary(b)
lm.best = summary(b)
which.min(lm.best$cp)
################################################################################
set.seed(1)
dataset2 = na.omit(dataset2)
#create train and test sets
train=sample(c(TRUE,FALSE),nrow(nyc),rep=TRUE)
test=(!train)
#dataset2
#Find best model by finding the error on the number of variables
regfit.fwd=regsubsets(IncomePerCap~., data=dataset2, nvmax=20,method='forward')
regfit.fwd.train=regsubsets(IncomePerCap~., data=dataset2[train,], nvmax = 20, method='forward')
test.mat=model.matrix(IncomePerCap~.,data=dataset2[test,])


val.errors=rep(NA,19)
for (i in 1:19){
  coef=coef(regfit.fwd.train,id=i)
  pred=test.mat[,names(coef)]%*%coef
  val.errors[i]=mean((dataset2$IncomePerCap[test]-pred)^2)
}
which.min(val.errors)
print('test errors' )
min(val.errors)
#best 7 predictors with the training dataset2
coef(regfit.fwd.train, 14)
#################################################################
# define predict function
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coef=coef(object,id=id)
  xvars=names(coef)
  mat[,xvars]%*%coef
}

#Cross Validate for best number of predictors in the full dataset2
set.seed(1)
k=5
folds=sample(1:k,nrow(dataset2),replace=TRUE)
cv.errors2=matrix(NA,k,19,dimnames=list(NULL,paste(1:19)))
for (j in 1:k){
  fwd.fit=regsubsets(IncomePerCap~.,data=dataset2[folds!=j,],nvmax=19)
  for (i in 1:19){
    pred=predict(fwd.fit,dataset2[folds==j,],id=i)
    cv.errors2[j,i]=mean((dataset2$IncomePerCap[folds==j]-pred)^2)
  }
}
mean.cv.errors2=apply(cv.errors2,2,mean)
# how many features are most important
which.min(mean.cv.errors2)
print(min(mean.cv.errors2))
#Cross validated reults give us 10 best features.
coef(fwd.fit,15)
```